{
 "cells": [
  {
   "cell_type": "raw",
   "id": "78d193ee-7ae6-4d50-bb6c-c72dc2962465",
   "metadata": {},
   "source": [
    "                                                FACE RECOGNITION PROJECT (Resnet50 + SVM)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28eb2646-3f26-4e6d-b9f6-528928adf32c",
   "metadata": {},
   "source": [
    "Quoc Dat Cao  301550055"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c547087c-38bd-4dde-baff-a9399b244839",
   "metadata": {},
   "source": [
    "Import useful library\n",
    "os:  \n",
    "    Used for interacting with the operating system.\n",
    "    Common operations include creating directories, navigating file paths, and managing files.\n",
    "random:\n",
    "    Provides functions to introduce randomness, such as shuffling lists or selecting random samples.\n",
    "\n",
    "shutil:\n",
    "    Used for high-level file operations, such as copying and moving files between directories.\n",
    "\n",
    "pandas (pd):\n",
    "    A powerful library for data manipulation and analysis.\n",
    "    Used to read, group, and process structured data (e.g., CSV files).\n",
    "\n",
    "numpy (np):\n",
    "    A fundamental library for numerical computing in Python.\n",
    "    Used to handle arrays and perform mathematical operations efficiently.\n",
    "\n",
    "joblib:\n",
    "    Used for saving and loading Python objects, such as machine learning models, to/from disk.\n",
    "\n",
    "seaborn (sns):\n",
    "    A data visualization library built on top of Matplotlib.\n",
    "    Used to create visually appealing charts, such as heatmaps for confusion matrices.\n",
    "\n",
    "matplotlib.pyplot (plt):\n",
    "    The foundational plotting library in Python.\n",
    "    Used to visualize data through graphs and charts.\n",
    "\n",
    "\n",
    "--------------------------------Deep Learning Library--------------------------------\n",
    "tensorflow.keras.applications.ResNet50:\n",
    "    Provides access to the ResNet-50 model, pre-trained on ImageNet.\n",
    "    Used for extracting features from images.\n",
    "\n",
    "tensorflow.keras.models.Model:\n",
    "    Used to define and manipulate Keras models.\n",
    "    Useful for creating custom models by specifying inputs and outputs.\n",
    "\n",
    "tensorflow.keras.applications.resnet50.preprocess_input:\n",
    "    Prepares image data to match the input requirements of the ResNet-50 model \n",
    "    (e.g., scaling pixel values).\n",
    "\n",
    "tensorflow.keras.preprocessing.image:\n",
    "    Contains utilities for image processing, such as loading images and converting them to arrays.\n",
    "\n",
    "tensorflow.keras.layers.GlobalAveragePooling2D:\n",
    "    A Keras layer used to reduce the dimensionality of the ResNet feature maps \n",
    "    by computing the average of each feature map.\n",
    "\n",
    "sklearn.svm.SVC:\n",
    "    Implements a Support Vector Classifier, which is used for training a classification model.\n",
    "\n",
    "sklearn.preprocessing.StandardScaler:\n",
    "    Used to scale features by removing the mean and scaling to unit variance.\n",
    "    Ensures that all features contribute equally to the SVM.\n",
    "\n",
    "sklearn.pipeline.make_pipeline:\n",
    "    Creates a pipeline to streamline the combination of preprocessing steps \n",
    "    (e.g., scaling) and the SVM model.\n",
    "\n",
    "sklearn.metrics:\n",
    "    Provides tools to evaluate model performance.\n",
    "    Includes metrics like accuracy, classification reports, and confusion matrices.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c96ae9-8c4e-4ef8-94d8-c5a1be3e50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 12:07:18.924892: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 12:07:19.081699: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a70965a-6665-432c-8266-163197adc130",
   "metadata": {},
   "source": [
    "First step is Data Preparation.\n",
    "    In this step, the number of image of each label(name of the person)'s data is separated into 80% for training and 20% for testing.\n",
    "    I also count the number of pictures a person has in the data to have an overall look of the dataset. The dataset was found on Kaggle from this link: https://www.kaggle.com/datasets/vasukipatel/face-recognition-dataset\n",
    "    \n",
    "    Person with the largest number of images: Brad Pitt (120 images)\n",
    "    Person with the smallest number of images: Kashyap (30 images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56fc0fa-65ae-4589-ab0e-ad32bef1ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image Counts for Each Person:\n",
      "Akshay Kumar: 50\n",
      "Alexandra Daddario: 92\n",
      "Alia Bhatt: 79\n",
      "Amitabh Bachchan: 74\n",
      "Andy Samberg: 92\n",
      "Anushka Sharma: 68\n",
      "Billie Eilish: 98\n",
      "Brad Pitt: 120\n",
      "Camila Cabello: 87\n",
      "Charlize Theron: 78\n",
      "Claire Holt: 96\n",
      "Courtney Cox: 80\n",
      "Dwayne Johnson: 61\n",
      "Elizabeth Olsen: 71\n",
      "Ellen Degeneres: 75\n",
      "Henry Cavill: 106\n",
      "Hrithik Roshan: 101\n",
      "Hugh Jackman: 112\n",
      "Jessica Alba: 108\n",
      "Kashyap: 30\n",
      "Lisa Kudrow: 70\n",
      "Margot Robbie: 72\n",
      "Marmik: 32\n",
      "Natalie Portman: 105\n",
      "Priyanka Chopra: 102\n",
      "Robert Downey Jr: 113\n",
      "Roger Federer: 77\n",
      "Tom Cruise: 58\n",
      "Vijay Deverakonda: 115\n",
      "Virat Kohli: 49\n",
      "Zac Efron: 91\n",
      "\n",
      "Person with the largest number of images: Brad Pitt (120 images)\n",
      "Person with the smallest number of images: Kashyap (30 images)\n",
      "\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPARATION\n",
    "# File paths\n",
    "data_folder = \"data\"\n",
    "train_folder = \"train\"\n",
    "test_folder = \"test\"\n",
    "csv_file_path = 'dataset.csv'\n",
    "\n",
    "# Create train and test directories\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Group images by label (person name)\n",
    "grouped = dataset.groupby(\"label\")\n",
    "\n",
    "# Dictionary to store the count of images for each person\n",
    "image_counts = {}\n",
    "\n",
    "for label, group in grouped:\n",
    "    # Get all image filenames for this person\n",
    "    images = group[\"id\"].tolist()\n",
    "    \n",
    "    # Count the number of images for this label\n",
    "    image_counts[label] = len(images)\n",
    "    \n",
    "    # Shuffle the images for randomness\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    # Split images: 80% to train, remaining to test\n",
    "    split_idx = int(0.8 * len(images))\n",
    "    train_images = images[:split_idx]\n",
    "    test_images = images[split_idx:]\n",
    "    \n",
    "    # Create label-specific folders in train and test\n",
    "    train_person_folder = os.path.join(train_folder, label)\n",
    "    test_person_folder = os.path.join(test_folder, label)\n",
    "    os.makedirs(train_person_folder, exist_ok=True)\n",
    "    os.makedirs(test_person_folder, exist_ok=True)\n",
    "    \n",
    "    # Move 80% images to train folder\n",
    "    for img in train_images:\n",
    "        src = os.path.join(data_folder, img)\n",
    "        dst = os.path.join(train_person_folder, img)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "    \n",
    "    # Move the remaining 20% to the test folder\n",
    "    for img in test_images:\n",
    "        src = os.path.join(data_folder, img)\n",
    "        dst = os.path.join(test_person_folder, img)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# Find the person with the largest and smallest number of images\n",
    "largest_person = max(image_counts, key=image_counts.get)\n",
    "smallest_person = min(image_counts, key=image_counts.get)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nImage Counts for Each Person:\")\n",
    "for label, count in image_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "print(f\"\\nPerson with the largest number of images: {largest_person} ({image_counts[largest_person]} images)\")\n",
    "print(f\"Person with the smallest number of images: {smallest_person} ({image_counts[smallest_person]} images)\")\n",
    "\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdf4e9c7-19ab-4907-be38-9cd4bfc88ea6",
   "metadata": {},
   "source": [
    "---------------------------------------------------FEATURE EXTRACTION-------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef9fa816-ea11-4c7d-aede-7e2a169463a4",
   "metadata": {},
   "source": [
    "Firstly, load the model Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e4f617-c387-46eb-b079-053f8e6848a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 12:07:23.720981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.774208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.774636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.775900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 12:07:23.777776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.778202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.778525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.876992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.877430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.877728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-02 12:07:23.877996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4267 MB memory:  -> device: 0, name: NVIDIA RTX A3000 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "imagesize_target=224\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(imagesize_target, imagesize_target, 3))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8a09d87-b022-4f13-a09e-7a009ac00e3c",
   "metadata": {},
   "source": [
    "    The original resnet model used 224x224 image size, during experiment with different size, which I give more detail in the report, I see that this image's size get better result although it cause slower in performance than smaller size. The original images in dataset have size 160x160. However, this size yield lower accuracy.\n",
    "\n",
    "    Let look at the model Resnet50!\n",
    "    ResNet50 is a deep convolutional neural network with 50 layers designed to handle image classification and feature extraction tasks.\n",
    "    It's a powerful architecture for transfer learning because it is pre-trained on the large ImageNet dataset and captures generic image features.\n",
    "\n",
    "    >> weights='imagenet' <<\n",
    "    It loads pre-trained weights learned from the ImageNet dataset, which contains over 14 million labeled images.\n",
    "    Transfer learning: Leverage pre-trained weights instead of training the network from scratch, saving time. These weights are optimized for common image patterns like edges, shapes, and textures.\n",
    "\n",
    "    >> include_top=False <<\n",
    "    It mean that I don't use the fully connected (dense) layers at the top of the network because I use svm classifier for the classification task, this I won't use this layers. Fully connected layers in the default ResNet50 are specific to the ImageNet classification task (1,000 classes). Excluding the top layers leaves only the convolutional base, which outputs feature maps that are general-purpose. This allows me to use ResNet50 for tasks like feature extraction or fine-tuning with my own classifier.\n",
    "\n",
    "    >> input_shape=(imagesize_target, imagesize_target, 3) <<\n",
    "    It specifies the shape of the input images, where I play with my parameter, change the size by adjusting the imagesize_target to experiment how it will affect the accuracy.3: The number of color channels (RGB).\n",
    "    Customizing imagesize_target:\n",
    "        Smaller sizes (e.g., 128x128) are faster to process and require less memory but may lose spatial detail lead to lower accuracy.\n",
    "        Larger sizes (e.g., 299x299) retain more detail but require more resource.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0544d2a9-4d27-4f14-b3e5-acce20c9d329",
   "metadata": {},
   "source": [
    "    The ImageDataGenerator object (test_datagen) preprocesses images to ensure they are compatible with the pre-trained ResNet50 model. Pixel values, which are originally in the range [0, 255], are normalized to a range suitable for ResNet50, specifically [-1, 1], and resize it to experiment imagesize_target.\n",
    "    >> batch_size=32 << specifies that 32 images are processed at a time. It balances memory usage and computation speed.\n",
    "    -> Smaller for limited memory.\n",
    "    -> Larger for faster training with sufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a411f999-74c6-44ea-9bbe-0b3ee3ef0ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2537 images belonging to 31 classes.\n",
      "Found 1261 images belonging to 31 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data generators for train and test sets\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_folder,\n",
    "    target_size=(imagesize_target, imagesize_target),\n",
    "    batch_size=32,  # You can adjust this based on your system's memory\n",
    "    class_mode=None,  # No labels required for feature extraction\n",
    "    shuffle=False  # Keep order consistent for mapping labels later\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_folder,\n",
    "    target_size=(imagesize_target, imagesize_target),\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ca70820-882e-4d19-ab39-25a32281cac3",
   "metadata": {},
   "source": [
    "The code below to map the person name to label's number for final manual visual testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d299060-4ee5-4439-83cb-b9ac3fada598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping saved!\n"
     ]
    }
   ],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "inverse_class_indices = {v: k for k, v in class_indices.items()}  # Reverse the dictionary\n",
    "joblib.dump(inverse_class_indices, \"class_mapping.pkl\")\n",
    "print(\"Class mapping saved!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "387b8f1e-f0c1-4b84-9c8f-e758ad1809bb",
   "metadata": {},
   "source": [
    "This code snippet is responsible for extracting features from images using a pre-trained ResNet50 model and saving them for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc109e7-fdef-41a6-b0b4-f7d72d0e5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 12:07:28.811431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 11s 95ms/step\n",
      "Training features saved!\n"
     ]
    }
   ],
   "source": [
    "# Extract features for train set\n",
    "print(\"Extracting features for training set...\")\n",
    "train_features = resnet_model.predict(train_generator, verbose=1)\n",
    "train_labels = train_generator.classes  # Class indices corresponding to features\n",
    "np.save(\"train_features.npy\", train_features)  # Save features\n",
    "np.save(\"train_labels.npy\", train_labels)  # Save labels\n",
    "print(\"Training features saved!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9dce420-8aa1-4a8e-b97b-02e0fdd5dec3",
   "metadata": {},
   "source": [
    "resnet_model.predict(train_generator):\n",
    "    Uses the pre-trained ResNet50 model (resnet_model) to process the images from the train_generator.\n",
    "    Each image is passed through the ResNet50 convolutional layers, which output high-dimensional feature maps for each image.\n",
    "\n",
    "train_generator.classes:\n",
    "    Retrieves the labels (class indices) for each image in the training set. \n",
    "    These are integer representations of the classes (e.g., 0, 1, 2, ...).\n",
    "\n",
    "np.save(\"train_features.npy\", train_features):\n",
    "    Saves the extracted features to a .npy file for later use.\n",
    "    These features can be loaded later without needing to re-extract them from ResNet50.\n",
    "\n",
    "np.save(\"train_labels.npy\", train_labels):\n",
    "    Saves the corresponding labels for the training images to another .npy file.\n",
    "\n",
    "Output:\n",
    "    The extracted features and their corresponding labels are stored in the files:\n",
    "        train_features.npy\n",
    "        train_labels.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd8c68d-8463-4f3a-b83c-ef853888829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for testing set...\n",
      "40/40 [==============================] - 5s 113ms/step\n",
      "Testing features saved!\n"
     ]
    }
   ],
   "source": [
    "# Extract features for test set\n",
    "print(\"Extracting features for testing set...\")\n",
    "test_features = resnet_model.predict(test_generator, verbose=1)\n",
    "test_labels = test_generator.classes  # Class indices corresponding to features\n",
    "np.save(\"test_features.npy\", test_features)  # Save features\n",
    "np.save(\"test_labels.npy\", test_labels)  # Save labels\n",
    "print(\"Testing features saved!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ab0d52a-662f-4f7d-9f4c-0fd73d8b4f23",
   "metadata": {},
   "source": [
    "resnet_model.predict(test_generator):\n",
    "    Processes the test images using the same ResNet50 model, extracting feature maps for each test image.\n",
    "    These features are used for evaluating the model's performance later.\n",
    "\n",
    "test_generator.classes:\n",
    "    Retrieves the class indices (integer labels) for the test images.\n",
    "\n",
    "Saving the Data:\n",
    "    Features are saved to test_features.npy.\n",
    "    Labels are saved to test_labels.npy.\n",
    "\n",
    "Output:\n",
    "    The extracted features and labels are stored in:\n",
    "        test_features.npy\n",
    "        test_labels.npy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6c3bd41-d4e4-45c3-9422-2bc3c0955065",
   "metadata": {},
   "source": [
    "    Global Average Pooling (GAP) is a crucial step in the pipeline because it prepares the feature maps output by ResNet50 for use in my refered machine learning classifier SVM.\n",
    "    ResNet50 Output Shape\n",
    "    When include_top=False, ResNet50 outputs a 4D tensor (feature maps) of shape:(num_samples, height, width, channels)\n",
    "    Traditional classifiers like SVM require 2D input, where each sample is represented as a single fixed-length vector\n",
    "    The 4D tensor from ResNet50 needs to be flattened into a 2D matrix:(num_samples, 2048)\n",
    "    GAP flattens the spatial dimensions (height and width) by computing the average of all pixel values in each channel.\n",
    "The resulting output has one value per channel, which represents the channel's overall \"activation\" or importance.\n",
    "This reduces the dimensionality of the feature map while retaining its semantic meaning.\n",
    "    If I don't use GAP and try to pass the 4D tensor ((7,7,2048)(7,7,2048)) to the SVM, it causes error in SVM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeeb5d1c-57bc-4f27-88e7-aa31d7509cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened features saved!\n"
     ]
    }
   ],
   "source": [
    "# Optionally, perform Global Average Pooling to flatten features\n",
    "gap = GlobalAveragePooling2D()\n",
    "train_features_flat = gap(train_features).numpy()  # Shape: (num_train_samples, 2048)\n",
    "test_features_flat = gap(test_features).numpy()    # Shape: (num_test_samples, 2048)\n",
    "\n",
    "# Save flattened features\n",
    "np.save(\"train_features_flat.npy\", train_features_flat)\n",
    "np.save(\"test_features_flat.npy\", test_features_flat)\n",
    "print(\"Flattened features saved!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ecab768-25c4-4165-9a93-314ba1446a5f",
   "metadata": {},
   "source": [
    "---------------------------------------------------SVM CLASSIFICATION-------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4905efd3-bf87-49ca-b265-c3b42030b96f",
   "metadata": {},
   "source": [
    "Firstly, reload the saved train and test features "
   ]
  },
  {
   "cell_type": "raw",
   "id": "67e68207-0baa-48f8-976b-79a9c6494039",
   "metadata": {},
   "source": [
    "make_pipeline(StandardScaler(), SVC(...)) creates a pipeline with:\n",
    "    >> StandardScaler() <<\n",
    "        Standardizes the feature data by removing the mean and scaling to unit variance.\n",
    "        Ensures that all features contribute equally, as SVMs are sensitive to the scale of features.\n",
    "    >> SVC <<\n",
    "        Implements a Support Vector Classifier (SVM).\n",
    "        Parameters:\n",
    "            kernel='linear': Uses a linear decision boundary (hyperplane) to separate classes.\n",
    "            C=1: Controls the regularization strength (trade-off between complexity and misclassification).\n",
    "            degree=1: Has no effect here since the kernel is linear (only relevant for polynomial kernels).\n",
    "            probability=True: Enables probability estimates for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c83971-c430-46cb-8beb-7719137353c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Saved Features and Labels\n",
    "print(\"Loading features and labels...\")\n",
    "train_features = np.load(\"train_features_flat.npy\")  # Flattened training features\n",
    "test_features = np.load(\"test_features_flat.npy\")    # Flattened testing features\n",
    "train_labels = np.load(\"train_labels.npy\")           # Training labels\n",
    "test_labels = np.load(\"test_labels.npy\")             # Testing labels\n",
    "print(\"Features and labels loaded successfully!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36c44c41-2533-46d5-ad37-2bbbe6aae6e5",
   "metadata": {},
   "source": [
    "svm_classifier.fit(train_features, train_labels):\n",
    "    Trains the SVM classifier using the training features (train_features) and their corresponding labels (train_labels).\n",
    "    The model learns a linear hyperplane to separate the classes in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958388e4-a488-4faa-8e62-28f868b22349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train the SVM Classifier\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear', probability=True))\n",
    "svm_classifier.fit(train_features, train_labels)\n",
    "print(\"SVM training complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88d0dea-8800-4024-8d30-8f6cd378b512",
   "metadata": {},
   "source": [
    "Step 3 and 4: calculate the accuracy of the svm model and save the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7bab8-0305-49b4-a990-dc09b1c82aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate the SVM Classifier\n",
    "print(\"Evaluating the classifier...\")\n",
    "test_predictions = svm_classifier.predict(test_features)\n",
    "# Calculate and display accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92db2a89-f893-4da2-a0f0-050869e10607",
   "metadata": {},
   "source": [
    "Compares the predicted labels (test_predictions) from my SVM model with the true labels (test_labels) in the test set.\n",
    "and have an overall Performance Evaluation:\n",
    "    Accuracy is a quick way to evaluate how well my model performs on the test set.\n",
    "    It shows model's correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76f444-a915-4524-9f9e-f8c8631566fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "# Step 4: Save the Trained SVM Model\n",
    "print(\"Saving the trained SVM model...\")\n",
    "joblib.dump(svm_classifier, \"svm_classifier.pkl\")\n",
    "resnet_model.save(\"resnet_model.h5\")\n",
    "print(\"SVM model saved as 'svm_classifier.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4426aad5-4e8c-43d0-a873-8c2525e190b5",
   "metadata": {},
   "source": [
    "----------------------------------------------------------TESTING-------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4b2bac9-7329-4844-bc91-f046147ea13d",
   "metadata": {},
   "source": [
    "Step 1 and 2: reload saved features and label and reuse the trained SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b5ec7-f078-4e1f-b541-8e2270c2ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Test Features and Labels\n",
    "print(\"Loading test features and labels...\")\n",
    "test_features = np.load(\"test_features_flat.npy\")  # Flattened test features\n",
    "test_labels = np.load(\"test_labels.npy\")          # Corresponding labels\n",
    "print(f\"Test features shape: {test_features.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Step 2: Load the Trained SVM Model\n",
    "print(\"Loading the trained SVM model...\")\n",
    "svm_classifier = joblib.load(\"svm_classifier.pkl\")\n",
    "print(\"SVM model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d98c8cef-0bac-4559-89f2-3dd91d3db2a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46d680-2adc-44da-bcaf-ccc2e8257f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make Predictions on Test Features\n",
    "print(\"Making predictions on the test dataset...\")\n",
    "test_predictions = svm_classifier.predict(test_features)\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1e817-b253-41cb-8888-6a1d5377c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 5: Visualize the Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=svm_classifier.classes_, yticklabels=svm_classifier.classes_)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Test on Individual Images\n",
    "# Load the saved ResNet model\n",
    "if not os.path.exists(\"resnet_model.h5\"):\n",
    "    raise FileNotFoundError(\"The ResNet model file 'resnet_model.h5' was not found.\")\n",
    "resnet_model = load_model(\"resnet_model.h5\")\n",
    "print(\"ResNet model loaded successfully!\")\n",
    "\n",
    "# Load the saved SVM model\n",
    "if not os.path.exists(\"svm_classifier.pkl\"):\n",
    "    raise FileNotFoundError(\"The SVM model file 'svm_classifier.pkl' was not found.\")\n",
    "svm_classifier = joblib.load(\"svm_classifier.pkl\")\n",
    "print(\"SVM model loaded successfully!\")\n",
    "\n",
    "# Load the class mapping\n",
    "if not os.path.exists(\"class_mapping.pkl\"):\n",
    "    raise FileNotFoundError(\"The class mapping file 'class_mapping.pkl' was not found.\")\n",
    "inverse_class_indices = joblib.load(\"class_mapping.pkl\")\n",
    "print(\"Class mapping loaded successfully!\")\n",
    "\n",
    "# Define function to predict a single image\n",
    "def predict_single_image(img_path, resnet_model, svm_model, class_mapping):\n",
    "    \"\"\"\n",
    "    Predict the label for a single image and return the class name.\n",
    "    :param img_path: Path to the image file.\n",
    "    :param resnet_model: Pre-trained ResNet model.\n",
    "    :param svm_model: Trained SVM classifier.\n",
    "    :param class_mapping: Dictionary mapping numeric labels to class names.\n",
    "    :return: Predicted class name.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    img = load_img(img_path, target_size=(imagesize_target, imagesize_target))  # Resize to ResNet input size\n",
    "    img_array = img_to_array(img)  # Convert to NumPy array\n",
    "    img_array = preprocess_input(np.expand_dims(img_array, axis=0))  # Add batch dimension and preprocess\n",
    "    \n",
    "    # Extract features using ResNet\n",
    "    print(f\"Extracting features for image: {img_path}\")\n",
    "    image_features = resnet_model.predict(img_array)  # Extract features\n",
    "    image_features_flat = image_features.mean(axis=(1, 2))  # Global Average Pooling\n",
    "\n",
    "    # Predict label using SVM\n",
    "    print(\"Predicting label...\")\n",
    "    numeric_label = svm_model.predict(image_features_flat)[0]\n",
    "    class_name = class_mapping[numeric_label]  # Map numeric label to class name\n",
    "    return class_name, img\n",
    "\n",
    "# Example: Predict and display an individual image\n",
    "img_path = \"b.png\"  # Replace with the path to a test image\n",
    "\n",
    "try:\n",
    "    # Predict the label\n",
    "    predicted_class_name, img = predict_single_image(img_path, resnet_model, svm_classifier, inverse_class_indices)\n",
    "    print(f\"The predicted name for the image '{img_path}' is: {predicted_class_name}\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.title(f\"Predicted: {predicted_class_name}\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
